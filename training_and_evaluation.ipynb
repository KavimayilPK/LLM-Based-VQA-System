{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cffg2i257iMS"
      },
      "source": [
        "# Image captioning with visual attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "cellView": "form",
        "id": "U8l4RJ0XRPEm"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import pathlib\n",
        "import re\n",
        "import string\n",
        "\n",
        "import einops\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tqdm\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqGXX9Dc5c0v"
      },
      "source": [
        "#### Flickr8k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "    \n",
        "def moveFiles(source_dir, target_dir):\n",
        "    file_names = os.listdir(source_dir)\n",
        "        \n",
        "    for file_name in file_names:\n",
        "        if file_name=='__MACOSX':\n",
        "            continue\n",
        "        shutil.move(os.path.join(source_dir, file_name), target_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "kaNy_l7tGuAZ"
      },
      "outputs": [],
      "source": [
        "def flickr8k(path='flickr8k'):\n",
        "  import shutil\n",
        "  import os\n",
        "\n",
        "  path = pathlib.Path(path)\n",
        "\n",
        "  if len(list(path.rglob('*'))) < 16197:\n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "    \n",
        "    moveFiles(str(path)+'/Flickr8k_Dataset.zip', str(path))\n",
        "    \n",
        "    tf.keras.utils.get_file(\n",
        "        origin='https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip',\n",
        "        cache_dir='.',\n",
        "        cache_subdir=path,\n",
        "        extract=True)\n",
        "    \n",
        "    moveFiles(str(path)+'/Flickr8k_text.zip', str(path))\n",
        "\n",
        "  captions = (path/\"Flickr8k.token.txt\").read_text().splitlines()\n",
        "  captions = (line.split('\\t') for line in captions)\n",
        "  captions = ((fname.split('#')[0], caption) for (fname, caption) in captions)\n",
        "\n",
        "  cap_dict = collections.defaultdict(list)\n",
        "  for fname, cap in captions:\n",
        "    cap_dict[fname].append(cap)\n",
        "\n",
        "  train_files = (path/'Flickr_8k.trainImages.txt').read_text().splitlines()\n",
        "  train_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in train_files]\n",
        "\n",
        "  test_files = (path/'Flickr_8k.testImages.txt').read_text().splitlines()\n",
        "  test_captions = [(str(path/'Flicker8k_Dataset'/fname), cap_dict[fname]) for fname in test_files]\n",
        "\n",
        "  train_ds = tf.data.experimental.from_list(train_captions)\n",
        "  test_ds = tf.data.experimental.from_list(test_captions)\n",
        "\n",
        "  return train_ds, test_ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBAagBw5p-TM"
      },
      "source": [
        "#### Download the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFtTZaobquNr"
      },
      "source": [
        "The Flickr8k is a good choice because it contains 5-captions per image, more data for a smaller download."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "EJySPbzJ4Wxw"
      },
      "outputs": [],
      "source": [
        "import ssl\n",
        "ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "train_raw, test_raw = flickr8k()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-UAc275FHxm8"
      },
      "source": [
        "The loaders for both datasets above return `tf.data.Dataset`s containing `(image_path, captions)` pairs. The Flickr8k dataset contains 5 captions per image, while Conceptual Captions has 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "sAQSps5F8RQI"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(TensorSpec(shape=(), dtype=tf.string, name=None),\n",
              " TensorSpec(shape=(5,), dtype=tf.string, name=None))"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_raw.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cSW4u-ORPFQ"
      },
      "source": [
        "### Image feature extractor \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "IlUckK8Zfikv"
      },
      "outputs": [],
      "source": [
        "IMAGE_SHAPE=(224, 224, 3)\n",
        "mobilenet = tf.keras.applications.MobileNetV3Small(\n",
        "    input_shape=IMAGE_SHAPE,\n",
        "    include_top=False,\n",
        "    include_preprocessing=True)\n",
        "mobilenet.trainable=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "zXR0217aRPFR"
      },
      "outputs": [],
      "source": [
        "def load_image(image_path):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.io.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, IMAGE_SHAPE[:-1])\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyqH3zFwRPFi"
      },
      "source": [
        "### Setup the text tokenizer/vectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 205,
      "metadata": {
        "id": "NroZIzB90hD3"
      },
      "outputs": [],
      "source": [
        "def standardize(s):\n",
        "  s = tf.strings.lower(s)\n",
        "  s = tf.strings.regex_replace(s, f'[{re.escape(string.punctuation)}]', '')\n",
        "  s = tf.strings.join(['[START]', s, '[END]'], separator=' ')\n",
        "  return s"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 206,
      "metadata": {
        "id": "n9SQOXFsyS36"
      },
      "outputs": [],
      "source": [
        "# Use the top 50000 words for a vocabulary.\n",
        "vocabulary_size = 50000\n",
        "tokenizer = tf.keras.layers.TextVectorization(\n",
        "    max_tokens=vocabulary_size,\n",
        "    standardize=standardize,\n",
        "    ragged=True)\n",
        "# Learn the vocabulary from the caption data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "oJGE34aiRPFo"
      },
      "outputs": [],
      "source": [
        "tokenizer.adapt(train_raw.map(lambda fp,txt: txt).unbatch().batch(1024))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "8Q44tNQVRPFt"
      },
      "outputs": [],
      "source": [
        "# Create mappings for words to indices and indices to words.\n",
        "word_to_index = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary())\n",
        "index_to_word = tf.keras.layers.StringLookup(\n",
        "    mask_token=\"\",\n",
        "    vocabulary=tokenizer.get_vocabulary(),\n",
        "    invert=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEWM9xrYcg45"
      },
      "source": [
        "### Prepare the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "3_Lqwl9NiGT0"
      },
      "outputs": [],
      "source": [
        "def match_shapes(images, captions):\n",
        "  caption_shape = einops.parse_shape(captions, 'b c')\n",
        "  captions = einops.rearrange(captions, 'b c -> (b c)')\n",
        "  images = einops.repeat(\n",
        "      images, 'b ... -> (b c) ...',\n",
        "      c = caption_shape['c'])\n",
        "  return images, captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 210,
      "metadata": {
        "id": "CZGUsuGzUfzt"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "image paths: (32,)\n",
            "captions: (32, 5)\n",
            "\n",
            "image_paths: (160,)\n",
            "captions: (160,)\n"
          ]
        }
      ],
      "source": [
        "for ex_paths, ex_captions in train_raw.batch(32).take(1):\n",
        "  break\n",
        "\n",
        "print('image paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n",
        "print()\n",
        "\n",
        "ex_paths, ex_captions = match_shapes(images=ex_paths, captions=ex_captions)\n",
        "\n",
        "print('image_paths:', ex_paths.shape)\n",
        "print('captions:', ex_captions.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 211,
      "metadata": {
        "id": "2DsgQ_hZT4C2"
      },
      "outputs": [],
      "source": [
        "def prepare_txt(imgs, txts):\n",
        "  tokens = tokenizer(txts)\n",
        "\n",
        "  input_tokens = tokens[..., :-1]\n",
        "  label_tokens = tokens[..., 1:]\n",
        "  return (imgs, input_tokens), label_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 212,
      "metadata": {
        "id": "4_Pt9zldjQ0q"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(ds, tokenizer, batch_size=32, shuffle_buffer=1000):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .shuffle(10000)\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  def to_tensor(inputs, labels):\n",
        "    (images, in_tok), out_tok = inputs, labels\n",
        "    return (images, in_tok.to_tensor()), out_tok.to_tensor()\n",
        "\n",
        "  return (ds\n",
        "          .map(match_shapes, tf.data.AUTOTUNE)\n",
        "          .unbatch()\n",
        "          .shuffle(shuffle_buffer)\n",
        "          .batch(batch_size)\n",
        "          .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "          .map(to_tensor, tf.data.AUTOTUNE)\n",
        "          )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 213,
      "metadata": {
        "id": "1KlhOG5cjQ0r"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 213,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds = prepare_dataset(train_raw, tokenizer)\n",
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "id": "d7Zy9F3zX7i2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 214,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_ds = prepare_dataset(test_raw, tokenizer)\n",
        "test_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZyKygJ8S8zW"
      },
      "source": [
        "### Cache the image features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 215,
      "metadata": {
        "id": "9N1MX5ym6xm5"
      },
      "outputs": [],
      "source": [
        "def save_dataset(ds, save_path, image_model, tokenizer, shards=10, batch_size=32):\n",
        "  # Load the images and make batches.\n",
        "  ds = (ds\n",
        "        .map(lambda path, caption: (load_image(path), caption))\n",
        "        .apply(tf.data.experimental.ignore_errors())\n",
        "        .batch(batch_size))\n",
        "\n",
        "  # Run the feature extractor on each batch\n",
        "  # Don't do this in a .map, because tf.data runs on the CPU. \n",
        "  def gen():\n",
        "    for (images, captions) in tqdm.tqdm(ds): \n",
        "      feature_maps = image_model(images)\n",
        "\n",
        "      feature_maps, captions = match_shapes(feature_maps, captions)\n",
        "      yield feature_maps, captions\n",
        "\n",
        "  # Wrap the generator in a new tf.data.Dataset.\n",
        "  new_ds = tf.data.Dataset.from_generator(\n",
        "      gen,\n",
        "      output_signature=(\n",
        "          tf.TensorSpec(shape=image_model.output_shape),\n",
        "          tf.TensorSpec(shape=(None,), dtype=tf.string)))\n",
        "\n",
        "  # Apply the tokenization \n",
        "  new_ds = (new_ds\n",
        "            .map(prepare_txt, tf.data.AUTOTUNE)\n",
        "            .unbatch()\n",
        "            .shuffle(1000))\n",
        "\n",
        "  # Save the dataset into shard files.\n",
        "  def shard_func(i, item):\n",
        "    return i % shards\n",
        "  new_ds.enumerate().save(save_path, shard_func=shard_func)\n",
        "\n",
        "def load_dataset(save_path, batch_size=32, shuffle=1000, cycle_length=2):\n",
        "  def custom_reader_func(datasets):\n",
        "    datasets = datasets.shuffle(1000)\n",
        "    return datasets.interleave(lambda x: x, cycle_length=cycle_length)\n",
        "  \n",
        "  ds = tf.data.Dataset.load(save_path, reader_func=custom_reader_func)\n",
        "\n",
        "  def drop_index(i, x):\n",
        "    return x\n",
        "\n",
        "  ds = (ds\n",
        "        .map(drop_index, tf.data.AUTOTUNE)\n",
        "        .shuffle(shuffle)\n",
        "        .padded_batch(batch_size)\n",
        "        .prefetch(tf.data.AUTOTUNE))\n",
        "  return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "tNdzrenxB3Yy"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "188it [01:15,  2.48it/s]\n",
            "32it [00:09,  3.36it/s]\n"
          ]
        }
      ],
      "source": [
        "save_dataset(train_raw, 'train_cache', mobilenet, tokenizer)\n",
        "save_dataset(test_raw, 'test_cache', mobilenet, tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "798DtfH51UI8"
      },
      "source": [
        " </section>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GI265LiDslr2"
      },
      "source": [
        "## Data ready for training\n",
        "\n",
        "After those preprocessing steps, here are the datasets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 217,
      "metadata": {
        "id": "Pwic2YCjHZmV"
      },
      "outputs": [],
      "source": [
        "train_ds = load_dataset('train_cache')\n",
        "test_ds = load_dataset('test_cache')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 218,
      "metadata": {
        "id": "3B80JXj7HloX"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((TensorSpec(shape=(None, 7, 7, 576), dtype=tf.float32, name=None),\n",
              "  TensorSpec(shape=(None, None), dtype=tf.int64, name=None)),\n",
              " TensorSpec(shape=(None, None), dtype=tf.int64, name=None))"
            ]
          },
          "execution_count": 218,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_ds.element_spec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfICM49WFpIb"
      },
      "source": [
        "## A Transformer decoder model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ngm3SQMCaYU"
      },
      "source": [
        "### Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {},
      "outputs": [],
      "source": [
        "def positional_encoding(length, depth):\n",
        "  depth = depth/2\n",
        "\n",
        "  positions = np.arange(length)[:, np.newaxis]     # (seq, 1)\n",
        "  depths = np.arange(depth)[np.newaxis, :]/depth   # (1, depth)\n",
        "\n",
        "  angle_rates = 1 / (10000**depths)         # (1, depth)\n",
        "  angle_rads = positions * angle_rates      # (pos, depth)\n",
        "\n",
        "  pos_encoding = np.concatenate(\n",
        "      [np.sin(angle_rads), np.cos(angle_rads)],\n",
        "      axis=-1) \n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PositionalEmbedding(tf.keras.layers.Layer):\n",
        "  def __init__(self, vocab_size, depth):\n",
        "    super().__init__()\n",
        "    self.d_model = depth\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, depth, mask_zero=True) \n",
        "    self.pos_encoding = positional_encoding(length=2048, depth=depth)\n",
        "\n",
        "  def compute_mask(self, *args, **kwargs):\n",
        "    return self.embedding.compute_mask(*args, **kwargs)\n",
        "\n",
        "  def call(self, x):\n",
        "    length = tf.shape(x)[1]\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
        "    x = x + self.pos_encoding[tf.newaxis, :length, :]\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "II1mD-bBCdMB"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "6JTLiX3lKooQ"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, **kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    # Use Add instead of + so the keras mask propagates through.\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    attn = self.mha(query=x, value=x,\n",
        "                    use_causal_mask=True)\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "id": "rIY6Vu2pLBAO"
      },
      "outputs": [],
      "source": [
        "class CrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,**kwargs):\n",
        "    super().__init__()\n",
        "    self.mha = tf.keras.layers.MultiHeadAttention(**kwargs)\n",
        "    self.add = tf.keras.layers.Add() \n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x, y, **kwargs):\n",
        "    attn, attention_scores = self.mha(\n",
        "             query=x, value=y,\n",
        "             return_attention_scores=True)\n",
        "    \n",
        "    self.last_attention_scores = attention_scores\n",
        "\n",
        "    x = self.add([x, attn])\n",
        "    return self.layernorm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {
        "id": "cWKrl7teOnH2"
      },
      "outputs": [],
      "source": [
        "class FeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    self.seq = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(units=2*units, activation='relu'),\n",
        "        tf.keras.layers.Dense(units=units),\n",
        "        tf.keras.layers.Dropout(rate=dropout_rate),\n",
        "    ])\n",
        "\n",
        "    self.layernorm = tf.keras.layers.LayerNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    x = x + self.seq(x)\n",
        "    return self.layernorm(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 224,
      "metadata": {
        "id": "ydcW5KZZHou7"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, units, num_heads=1, dropout_rate=0.1):\n",
        "    super().__init__()\n",
        "    \n",
        "    self.self_attention = CausalSelfAttention(num_heads=num_heads,\n",
        "                                              key_dim=units,\n",
        "                                              dropout=dropout_rate)\n",
        "    self.cross_attention = CrossAttention(num_heads=num_heads,\n",
        "                                          key_dim=units,\n",
        "                                          dropout=dropout_rate)\n",
        "    self.ff = FeedForward(units=units, dropout_rate=dropout_rate)\n",
        "      \n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    in_seq, out_seq = inputs\n",
        "\n",
        "    # Text input\n",
        "    out_seq = self.self_attention(out_seq)\n",
        "\n",
        "    out_seq = self.cross_attention(out_seq, in_seq)\n",
        "    \n",
        "    self.last_attention_scores = self.cross_attention.last_attention_scores\n",
        "\n",
        "    out_seq = self.ff(out_seq)\n",
        "\n",
        "    return out_seq"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lgbYrF5Csqu"
      },
      "source": [
        "### Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "id": "CeWw2SFDHUfo"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "class TokenOutput(tf.keras.layers.Layer):\n",
        "  def __init__(self, tokenizer, banned_tokens=('', '[UNK]', '[START]'), **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    vocab_size = tokenizer.vocabulary_size()\n",
        "    self.dense = tf.keras.layers.Dense(\n",
        "        units=vocab_size, **kwargs)\n",
        "    self.tokenizer = tokenizer\n",
        "    self.banned_tokens = banned_tokens\n",
        "\n",
        "    self.bias = None\n",
        "\n",
        "  def adapt(self, ds):\n",
        "    counts = collections.Counter()\n",
        "    vocab_dict = {name: id \n",
        "                  for id, name in enumerate(self.tokenizer.get_vocabulary())}\n",
        "\n",
        "    for tokens in tqdm.tqdm(ds):\n",
        "      counts.update(tokens.numpy().flatten())\n",
        "\n",
        "    counts_arr = np.zeros(shape=(self.tokenizer.vocabulary_size(),))\n",
        "    counts_arr[np.array(list(counts.keys()), dtype=np.int32)] = list(counts.values())\n",
        "\n",
        "    counts_arr = counts_arr[:]\n",
        "    for token in self.banned_tokens:\n",
        "      counts_arr[vocab_dict[token]] = 0\n",
        "\n",
        "    total = counts_arr.sum()\n",
        "    p = counts_arr/total\n",
        "    p[counts_arr==0] = 1.0\n",
        "    log_p = np.log(p)  # log(1) == 0\n",
        "\n",
        "    entropy = -(log_p*p).sum()\n",
        "\n",
        "    print()\n",
        "    print(f\"Uniform entropy: {np.log(self.tokenizer.vocabulary_size()):0.2f}\")\n",
        "    print(f\"Marginal entropy: {entropy:0.2f}\")\n",
        "\n",
        "    self.bias = log_p\n",
        "    self.bias[counts_arr==0] = -1e9\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    return x + self.bias\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzQHqANd1A6Q"
      },
      "source": [
        "The smart initialization will significantly reduce the initial loss:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "GGnOQyc501B2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/938 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 938/938 [00:06<00:00, 145.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Uniform entropy: 8.94\n",
            "Marginal entropy: 5.36\n"
          ]
        }
      ],
      "source": [
        "output_layer = TokenOutput(tokenizer, banned_tokens=('', '[UNK]', '[START]'))\n",
        "# This might run a little faster if the dataset didn't also have to load the image data.\n",
        "output_layer.adapt(train_ds.map(lambda inputs, labels: labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3gq-ICN7bD-u"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gou4fPH_SWgH"
      },
      "source": [
        "To build the model, you need to combine several parts:\n",
        "\n",
        "1. The image `feature_extractor` and the text `tokenizer` and.\n",
        "1. The `seq_embedding` layer, to convert batches of token-IDs to \n",
        "   vectors `(batch, sequence, channels)`.\n",
        "3. The stack of `DecoderLayers` layers that will process the text and image data.\n",
        "4. The `output_layer` which returns a pointwise prediction of what the next word should be."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 227,
      "metadata": {
        "id": "bHCISYehH1f6"
      },
      "outputs": [],
      "source": [
        "class Captioner(tf.keras.Model):\n",
        "  @classmethod\n",
        "  def add_method(cls, fun):\n",
        "    setattr(cls, fun.__name__, fun)\n",
        "    return fun\n",
        "\n",
        "  def __init__(self, tokenizer, feature_extractor, output_layer, num_layers=1,\n",
        "               units=256, max_length=50, num_heads=1, dropout_rate=0.1, **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    \n",
        "    self.max_length = max_length\n",
        "\n",
        "    self.feature_extractor = feature_extractor\n",
        "    self.tokenizer = tokenizer\n",
        "    self.word_to_index = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary())\n",
        "    self.index_to_word = tf.keras.layers.StringLookup(\n",
        "        mask_token=\"\",\n",
        "        vocabulary=tokenizer.get_vocabulary(),\n",
        "        invert=True) \n",
        "    \n",
        "    self.seq_embedding = PositionalEmbedding(\n",
        "        vocab_size=tokenizer.vocabulary_size(),\n",
        "        depth=units)\n",
        "\n",
        "    self.decoder_layers = [\n",
        "        DecoderLayer(units, num_heads=num_heads, dropout_rate=dropout_rate)\n",
        "        for n in range(num_layers)]\n",
        "\n",
        "    self.output_layer = output_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {
        "id": "lPdb7I4h9Ulo"
      },
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def call(self, inputs):\n",
        "  image, txt = inputs\n",
        "\n",
        "  if image.shape[-1] == 3:\n",
        "    # Apply the feature-extractor, if you get an RGB image.\n",
        "    image = self.feature_extractor(image)\n",
        "  \n",
        "  # Flatten the feature map\n",
        "  image = einops.rearrange(image, 'b h w c -> b (h w) c')\n",
        "\n",
        "\n",
        "  if txt.dtype == tf.string:\n",
        "    # Apply the tokenizer if you get string inputs.\n",
        "    txt = tokenizer(txt)\n",
        "\n",
        "  txt = self.seq_embedding(txt)\n",
        "\n",
        "  # Look at the image\n",
        "  for dec_layer in self.decoder_layers:\n",
        "    txt = dec_layer(inputs=(image, txt))\n",
        "    \n",
        "  txt = self.output_layer(txt)\n",
        "\n",
        "  return txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 266,
      "metadata": {
        "id": "kmM7aZQsLiyU"
      },
      "outputs": [],
      "source": [
        "model = Captioner(tokenizer, feature_extractor=mobilenet, output_layer=output_layer,\n",
        "                  units=256, max_length=100, dropout_rate=0.5, num_layers=3, num_heads=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGvOcLQKghXN"
      },
      "source": [
        "### Generate captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 267,
      "metadata": {
        "id": "cwFcdMqC-jE2"
      },
      "outputs": [],
      "source": [
        "image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 268,
      "metadata": {},
      "outputs": [],
      "source": [
        "@Captioner.add_method\n",
        "def simple_gen(self, image, temperature=1, initial_keywords=[]):\n",
        "\n",
        "  initial = self.word_to_index([['[START]']]) # (batch, sequence)\n",
        "\n",
        "  initial_keywords = ['[START]'] + initial_keywords\n",
        "  initial = self.word_to_index([initial_keywords]) # (batch, sequence)\n",
        "\n",
        "  img_features = self.feature_extractor(image[tf.newaxis, ...])\n",
        "\n",
        "  tokens = initial # (batch, sequence)\n",
        "\n",
        "  for n in range(self.max_length):\n",
        "    preds = self((img_features, tokens)).numpy()  # (batch, sequence, vocab)\n",
        "    preds = preds[:,-1, :]  #(batch, vocab)\n",
        "    if temperature==0:\n",
        "        next = tf.argmax(preds, axis=-1)[:, tf.newaxis]  # (batch, 1)\n",
        "    else:\n",
        "        next = tf.random.categorical(preds/temperature, num_samples=1)  # (batch, 1)\n",
        "\n",
        "    tokens = tf.concat([tokens, next], axis=1) # (batch, sequence)\n",
        "\n",
        "    if next[0] == self.word_to_index('[END]'):\n",
        "      break\n",
        "\n",
        "  words = index_to_word(tokens[0, 1:-1])\n",
        "  result = tf.strings.reduce_join(words, axis=-1, separator=' ')\n",
        "  return result.numpy().decode()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0FpTvaPkqON"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKcwZdqObK-U"
      },
      "source": [
        "To train the model you'll need several additional components:\n",
        "\n",
        "- The Loss and metrics\n",
        "- The Optimizer\n",
        "- Optional Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5IW2mWa2sAG"
      },
      "source": [
        "### Losses and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbpbDQTw1lOW"
      },
      "source": [
        "Here's an implementation of a masked loss and accuracy:\n",
        "\n",
        "When calculating the mask for the loss, note the `loss < 1e8`. This term discards the artificial, impossibly high losses for the `banned_tokens`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 269,
      "metadata": {
        "id": "s24im3FqxAfT"
      },
      "outputs": [],
      "source": [
        "def masked_loss(labels, preds):  \n",
        "  # print(labels)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "\n",
        "  loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels, preds)\n",
        "\n",
        "  mask = (labels != 0) & (loss < 1e8) \n",
        "  mask = tf.cast(mask, loss.dtype)\n",
        "\n",
        "  loss = loss*mask\n",
        "  loss = tf.reduce_sum(loss)/tf.reduce_sum(mask)\n",
        "  return loss\n",
        "\n",
        "def masked_acc(labels, preds):\n",
        "  mask = tf.cast(labels!=0, tf.float32)\n",
        "  preds = tf.argmax(preds, axis=-1)\n",
        "  labels = tf.cast(labels, tf.int64)\n",
        "  match = tf.cast(preds == labels, mask.dtype)\n",
        "  acc = tf.reduce_sum(match*mask)/tf.reduce_sum(mask)\n",
        "  return acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOhjHqgv3F2e"
      },
      "source": [
        "### Callbacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dyQN9UfJYEd"
      },
      "source": [
        "For feedback during training setup a `keras.callbacks.Callback` to generate some captions for the surfer image at the end of each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 270,
      "metadata": {
        "id": "IKDwbZOCZ-AP"
      },
      "outputs": [],
      "source": [
        "class GenerateText(tf.keras.callbacks.Callback):\n",
        "  def __init__(self):\n",
        "    image_url = 'https://tensorflow.org/images/surf.jpg'\n",
        "    image_path = tf.keras.utils.get_file('surf.jpg', origin=image_url)\n",
        "    self.image = load_image(image_path)\n",
        "\n",
        "  def on_epoch_end(self, epochs=None, logs=None):\n",
        "    print()\n",
        "    print()\n",
        "    for t in (0.0, 0.5, 1.0):\n",
        "      result = self.model.simple_gen(self.image, temperature=t)\n",
        "      print(result)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAxp4KZRKDk9"
      },
      "source": [
        "Also use `callbacks.EarlyStopping` to terminate training when the model starts to overfit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 271,
      "metadata": {
        "id": "MjzrwGZp23xx"
      },
      "outputs": [],
      "source": [
        "callbacks = [\n",
        "    GenerateText(),\n",
        "    tf.keras.callbacks.EarlyStopping(\n",
        "        patience=10, restore_best_weights=True)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBaJhQpcG8u0"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 272,
      "metadata": {
        "id": "2OR5ZpAII__u"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "           loss=masked_loss,\n",
        "           metrics=[masked_acc])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 273,
      "metadata": {
        "id": "3aB0baOVMZe9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:396: UserWarning: `build()` was called on layer 'captioner_3', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
            "  warnings.warn(\n",
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'causal_self_attention_7' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_layer_7' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 4.6360 - masked_acc: 0.2097\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:908: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "a boy in a blue shirt is in a water\n",
            "a little boy is in a red and white swimsuit\n",
            "a girl is sitting down the water pool\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 1s/step - loss: 4.6321 - masked_acc: 0.2101 - val_loss: 3.7375 - val_masked_acc: 0.3067\n",
            "Epoch 2/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 849ms/step - loss: 3.7146 - masked_acc: 0.3126\n",
            "\n",
            "a man in a blue shirt is in the water\n",
            "a young boy wearing a blue pants is jumping into a wave\n",
            "dogs playfully in the pool\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 976ms/step - loss: 3.7139 - masked_acc: 0.3127 - val_loss: 3.5811 - val_masked_acc: 0.3264\n",
            "Epoch 3/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 992ms/step - loss: 3.4997 - masked_acc: 0.3376\n",
            "\n",
            "a man in a blue shirt is in the water\n",
            "a man in a blue wetsuit on the water\n",
            "a man and the man wearing a yellow and standing on a pool\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 3.4996 - masked_acc: 0.3377 - val_loss: 3.4810 - val_masked_acc: 0.3458\n",
            "Epoch 4/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 874ms/step - loss: 3.3901 - masked_acc: 0.3552\n",
            "\n",
            "a man in a blue shirt is swimming in the water\n",
            "a man in a white wetsuit is surfing a wave\n",
            "a in trunks wave of a boat beside a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 989ms/step - loss: 3.3899 - masked_acc: 0.3552 - val_loss: 3.2765 - val_masked_acc: 0.3596\n",
            "Epoch 5/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 928ms/step - loss: 3.3080 - masked_acc: 0.3658\n",
            "\n",
            "a man in a red shirt is surfing a wave\n",
            "a man in a red hat with a surfboard\n",
            "a man is playing each picture in the air into the water\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - loss: 3.3078 - masked_acc: 0.3659 - val_loss: 3.2198 - val_masked_acc: 0.3677\n",
            "Epoch 6/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 695ms/step - loss: 3.2178 - masked_acc: 0.3793\n",
            "\n",
            "a man in a red shirt is surfing in the ocean\n",
            "a surfer in a yellow wetsuit is in a swimming pool\n",
            "a boy in a waves on a huge surfboard\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 768ms/step - loss: 3.2177 - masked_acc: 0.3793 - val_loss: 3.1382 - val_masked_acc: 0.3718\n",
            "Epoch 7/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 713ms/step - loss: 3.2097 - masked_acc: 0.3750\n",
            "\n",
            "a man in a blue shirt is surfing in the water\n",
            "a surfer in a blue shirt is riding a wave\n",
            "two young boys wrestlers splash in the water\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m78s\u001b[0m 785ms/step - loss: 3.2095 - masked_acc: 0.3750 - val_loss: 3.1362 - val_masked_acc: 0.3764\n",
            "Epoch 8/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 755ms/step - loss: 3.1689 - masked_acc: 0.3806\n",
            "\n",
            "a man in a red shirt is surfing a wave\n",
            "a man is surfing a wave\n",
            "a surfer holding a wave with a swimming surfboard\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 861ms/step - loss: 3.1687 - masked_acc: 0.3806 - val_loss: 3.0558 - val_masked_acc: 0.3845\n",
            "Epoch 9/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 971ms/step - loss: 3.1166 - masked_acc: 0.3838\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a man in the blue wetsuit is surfing at a wave\n",
            "a surfer kicks the splashes into the wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m108s\u001b[0m 1s/step - loss: 3.1168 - masked_acc: 0.3838 - val_loss: 3.1525 - val_masked_acc: 0.3752\n",
            "Epoch 10/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 886ms/step - loss: 3.1182 - masked_acc: 0.3894\n",
            "\n",
            "a man in a yellow wetsuit is surfing\n",
            "a man in a blue wetsuit is on a surfboard\n",
            "a boat skier in the goggles is surfing playing the wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 970ms/step - loss: 3.1174 - masked_acc: 0.3894 - val_loss: 3.0913 - val_masked_acc: 0.3798\n",
            "Epoch 11/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 870ms/step - loss: 2.9467 - masked_acc: 0.4007\n",
            "\n",
            "a man in a red shirt is surfing a wave\n",
            "a person in a blue wetsuit is a wave\n",
            "one man a person wings a bow in the wave on a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m96s\u001b[0m 959ms/step - loss: 2.9469 - masked_acc: 0.4007 - val_loss: 3.0078 - val_masked_acc: 0.3858\n",
            "Epoch 12/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901ms/step - loss: 2.9325 - masked_acc: 0.4079\n",
            "\n",
            "a surfer in a red and white wetsuit is in the water\n",
            "a man in red and a wetsuit is in a kayak\n",
            "a clown wave and wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 992ms/step - loss: 2.9325 - masked_acc: 0.4079 - val_loss: 3.0725 - val_masked_acc: 0.3865\n",
            "Epoch 13/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 973ms/step - loss: 2.9277 - masked_acc: 0.4082\n",
            "\n",
            "a surfer in a red shirt is surfing a wave\n",
            "a surfer in a yellow shirt is surfing on a wave\n",
            "a boat is going through the ocean into one battle wave and smiles\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.9275 - masked_acc: 0.4082 - val_loss: 3.0260 - val_masked_acc: 0.3831\n",
            "Epoch 14/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.9540 - masked_acc: 0.4037\n",
            "\n",
            "a surfer in a red shirt is surfing\n",
            "a surfer in a white wetsuit is in the ocean\n",
            "person wearing a checks a stick in the harness in the ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 1s/step - loss: 2.9536 - masked_acc: 0.4037 - val_loss: 2.9632 - val_masked_acc: 0.3915\n",
            "Epoch 15/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901ms/step - loss: 2.8754 - masked_acc: 0.4182\n",
            "\n",
            "a surfer in a wetsuit is surfing\n",
            "a surfer in a wetsuit in a blue boat in the ocean\n",
            "two people ride a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 984ms/step - loss: 2.8754 - masked_acc: 0.4182 - val_loss: 3.0068 - val_masked_acc: 0.3855\n",
            "Epoch 16/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 791ms/step - loss: 2.8321 - masked_acc: 0.4166\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a person in a white wetsuit is surfing\n",
            "this bearded kayak in a red surfer on by something\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 866ms/step - loss: 2.8322 - masked_acc: 0.4166 - val_loss: 2.9936 - val_masked_acc: 0.3932\n",
            "Epoch 17/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 773ms/step - loss: 2.8248 - masked_acc: 0.4232\n",
            "\n",
            "a surfer in a wetsuit is surfing\n",
            "a person in a blue wetsuit surfs\n",
            "a child with a wave life wetsuit in the foreground\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 846ms/step - loss: 2.8250 - masked_acc: 0.4232 - val_loss: 2.9711 - val_masked_acc: 0.3954\n",
            "Epoch 18/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 784ms/step - loss: 2.8393 - masked_acc: 0.4162\n",
            "\n",
            "a surfer is surfing a wave\n",
            "a surfer rides a wave\n",
            "a man plays cricket in the ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 855ms/step - loss: 2.8392 - masked_acc: 0.4162 - val_loss: 2.9723 - val_masked_acc: 0.3943\n",
            "Epoch 19/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 783ms/step - loss: 2.8452 - masked_acc: 0.4226\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer in a white wetsuit and yellow shirt is being pulled by the ocean\n",
            "there is a white surfboard\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 858ms/step - loss: 2.8450 - masked_acc: 0.4226 - val_loss: 2.9339 - val_masked_acc: 0.3940\n",
            "Epoch 20/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 968ms/step - loss: 2.7353 - masked_acc: 0.4288\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a man in a red wetsuit is riding a wave\n",
            "a belongings shining in the crashing side and in the the sky from the wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m106s\u001b[0m 1s/step - loss: 2.7352 - masked_acc: 0.4289 - val_loss: 2.8523 - val_masked_acc: 0.4158\n",
            "Epoch 21/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.7098 - masked_acc: 0.4419\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a person in a red wetsuit riding a wave\n",
            "the man wearing the blue and yellow wetsuit is driving in the crowd of a blue surfboard\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 1s/step - loss: 2.7095 - masked_acc: 0.4419 - val_loss: 2.9647 - val_masked_acc: 0.3968\n",
            "Epoch 22/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.6687 - masked_acc: 0.4385\n",
            "\n",
            "a man in a wetsuit is surfing a wave\n",
            "a man wearing a helmet is surfing in the ocean\n",
            "surfer her riders without a canoe flying with a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m148s\u001b[0m 1s/step - loss: 2.6689 - masked_acc: 0.4384 - val_loss: 2.9078 - val_masked_acc: 0.3975\n",
            "Epoch 23/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 901ms/step - loss: 2.6834 - masked_acc: 0.4454\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a man in a yellow and white shirt is in the surf\n",
            "a man in a red vest is riding a surfboard in a white boat\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 995ms/step - loss: 2.6832 - masked_acc: 0.4454 - val_loss: 3.0196 - val_masked_acc: 0.3918\n",
            "Epoch 24/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 2.6675 - masked_acc: 0.4384\n",
            "\n",
            "a man in a red and white wetsuit is surfing\n",
            "a boy in a red shirt is surfing\n",
            "a kid dressed in a red and white vest and trunks with a flip in the back\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m140s\u001b[0m 1s/step - loss: 2.6675 - masked_acc: 0.4384 - val_loss: 2.9394 - val_masked_acc: 0.3981\n",
            "Epoch 25/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 940ms/step - loss: 2.6592 - masked_acc: 0.4420\n",
            "\n",
            "a surfer in a red wetsuit is surfing a wave\n",
            "a surfer in a red shirt rides a wave\n",
            "a boy races his eyes\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 1s/step - loss: 2.6592 - masked_acc: 0.4420 - val_loss: 2.8901 - val_masked_acc: 0.3945\n",
            "Epoch 26/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 985ms/step - loss: 2.6359 - masked_acc: 0.4418\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a person in a blue wetsuit is surfing\n",
            "a woman waves floaties at a blue stage in the ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m109s\u001b[0m 1s/step - loss: 2.6359 - masked_acc: 0.4418 - val_loss: 2.9267 - val_masked_acc: 0.4020\n",
            "Epoch 27/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 827ms/step - loss: 2.6386 - masked_acc: 0.4473\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a person is surfing on a wave\n",
            "a yellow surfs on a surfboard in the ocean\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m91s\u001b[0m 906ms/step - loss: 2.6384 - masked_acc: 0.4473 - val_loss: 2.8507 - val_masked_acc: 0.4071\n",
            "Epoch 28/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 911ms/step - loss: 2.6274 - masked_acc: 0.4496\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer on a surfboard in the ocean\n",
            "a young woman wearing a red wetsuit rides his hands in the water\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 1s/step - loss: 2.6273 - masked_acc: 0.4496 - val_loss: 2.8928 - val_masked_acc: 0.4041\n",
            "Epoch 29/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 883ms/step - loss: 2.5267 - masked_acc: 0.4620\n",
            "\n",
            "a man in a red wetsuit is surfing on a wave\n",
            "a surfer in a red wetsuit is riding a wave in the ocean\n",
            "a blue slide in a white pool\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 976ms/step - loss: 2.5264 - masked_acc: 0.4620 - val_loss: 2.9170 - val_masked_acc: 0.4061\n",
            "Epoch 30/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 914ms/step - loss: 2.4192 - masked_acc: 0.4812\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a surfer rides a wave\n",
            "a man surfing in the dog\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m101s\u001b[0m 1s/step - loss: 2.4196 - masked_acc: 0.4811 - val_loss: 2.8315 - val_masked_acc: 0.4114\n",
            "Epoch 31/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 937ms/step - loss: 2.4485 - masked_acc: 0.4644\n",
            "\n",
            "a surfer in a white wetsuit is surfing\n",
            "a surfer in a red wetsuit is riding a wave\n",
            "a man surfing a trick on a board\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m102s\u001b[0m 1s/step - loss: 2.4486 - masked_acc: 0.4645 - val_loss: 2.8855 - val_masked_acc: 0.4024\n",
            "Epoch 32/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 889ms/step - loss: 2.4770 - masked_acc: 0.4726\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a man in a red wetsuit rides his board through the ocean\n",
            "a man wearing a vest and yellow board in the street in a clear flying\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 981ms/step - loss: 2.4771 - masked_acc: 0.4725 - val_loss: 2.8805 - val_masked_acc: 0.4052\n",
            "Epoch 33/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884ms/step - loss: 2.4588 - masked_acc: 0.4731\n",
            "\n",
            "a man in a red wetsuit is surfing\n",
            "a person in a red wetsuit rides a wave\n",
            "a boy floating in a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 973ms/step - loss: 2.4592 - masked_acc: 0.4731 - val_loss: 2.8990 - val_masked_acc: 0.3958\n",
            "Epoch 34/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9s/step - loss: 2.4984 - masked_acc: 0.4625\n",
            "\n",
            "a surfer in a red wetsuit is surfing on a wave\n",
            "a surfer in a red wetsuit surfs on a wave\n",
            "a boats is in a large wave at the wetsuit of a sandy tower\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m906s\u001b[0m 9s/step - loss: 2.4984 - masked_acc: 0.4625 - val_loss: 2.8529 - val_masked_acc: 0.4013\n",
            "Epoch 35/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 346ms/step - loss: 2.4334 - masked_acc: 0.4761\n",
            "\n",
            "a man in a red shirt and yellow kayak\n",
            "a man is surfing on a wave\n",
            "child riding on a wave\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 387ms/step - loss: 2.4333 - masked_acc: 0.4761 - val_loss: 2.8412 - val_masked_acc: 0.4106\n",
            "Epoch 36/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 344ms/step - loss: 2.4284 - masked_acc: 0.4737\n",
            "\n",
            "a man in a red shirt is surfing in a wave\n",
            "a person in a red and red shirt is surfing in a wave\n",
            "a man is riding in a kayak\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 382ms/step - loss: 2.4287 - masked_acc: 0.4737 - val_loss: 2.9424 - val_masked_acc: 0.3885\n",
            "Epoch 37/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 378ms/step - loss: 2.4608 - masked_acc: 0.4657\n",
            "\n",
            "a man in a red wetsuit is surfing on a wave\n",
            "a person in a black wetsuit surfs on a wave\n",
            "a man on a wave with its easel turned to night\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 420ms/step - loss: 2.4607 - masked_acc: 0.4658 - val_loss: 2.9039 - val_masked_acc: 0.3973\n",
            "Epoch 38/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 343ms/step - loss: 2.4400 - masked_acc: 0.4707\n",
            "\n",
            "a surfer in a red wetsuit is surfing on a wave\n",
            "a man in a red and white wetsuit surfs on a wave\n",
            "an older surfer in a white canoe in a river\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 384ms/step - loss: 2.4395 - masked_acc: 0.4708 - val_loss: 2.9206 - val_masked_acc: 0.4029\n",
            "Epoch 39/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 372ms/step - loss: 2.2372 - masked_acc: 0.5032\n",
            "\n",
            "a surfer in a red wetsuit is surfing\n",
            "a man in a red wetsuit surfs on a body of water\n",
            "a man surfing with a boogie board in his hair and white\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 414ms/step - loss: 2.2374 - masked_acc: 0.5032 - val_loss: 2.8682 - val_masked_acc: 0.3957\n",
            "Epoch 40/100\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 404ms/step - loss: 2.2855 - masked_acc: 0.4977\n",
            "\n",
            "a surfer in a red shirt is surfing on a wave\n",
            "a man in a red shirt is surfing on a wave\n",
            "a person is examining a wave on a wave with two left board\n",
            "\n",
            "\u001b[1m100/100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 456ms/step - loss: 2.2855 - masked_acc: 0.4977 - val_loss: 2.8823 - val_masked_acc: 0.4023\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(\n",
        "    train_ds.repeat(),\n",
        "    steps_per_epoch=100,\n",
        "    validation_data=test_ds.repeat(),\n",
        "    validation_steps=20,\n",
        "    epochs=100,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P634LfVgw-eV"
      },
      "source": [
        "Plot the loss and accuracy over the training run:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 274,
      "metadata": {
        "id": "6Wn8KSkUw916"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"loss.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 275,
      "metadata": {
        "id": "yZQ78b2Kxw-T"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "plt.plot(history.history['masked_acc'], label='accuracy')\n",
        "plt.plot(history.history['val_masked_acc'], label='val_accuracy')\n",
        "plt.ylim([0, max(plt.ylim())])\n",
        "plt.xlabel('Epoch #')\n",
        "plt.ylabel('CE/token')\n",
        "plt.legend()\n",
        "\n",
        "plt.savefig(\"acc.pdf\", bbox_inches='tight')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 276,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import urllib.request\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "import tensorflow as tf\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "# from rouge_score import rouge_scorer\n",
        "from rouge_score.rouge_scorer import RougeScorer\n",
        "from bert_score import score as bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## Downloading dataset for evaluation\n",
        "\n",
        "def download_file(url, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        print(f\"Downloading: {url}\")\n",
        "        urllib.request.urlretrieve(url, save_path)\n",
        "        print(f\"Saved to: {save_path}\")\n",
        "    else:\n",
        "        print(f\"Already downloaded: {save_path}\")\n",
        "\n",
        "# === Create download directory\n",
        "os.makedirs(\"downloads\", exist_ok=True)\n",
        "\n",
        "# === Download URLs\n",
        "downloads = {\n",
        "    \"val2014.zip\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
        "    \"annotations_trainval2014.zip\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "    \"v2_Questions_Val_mscoco.zip\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip\",\n",
        "    \"v2_Annotations_Val_mscoco.zip\": \"https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip\",\n",
        "}\n",
        "\n",
        "# === Download files\n",
        "for filename, url in downloads.items():\n",
        "    download_file(url, f\"downloads/{filename}\")\n",
        "\n",
        "# === Unzip files\n",
        "unzip_targets = {\n",
        "    \"downloads/val2014.zip\": \"val2014\",\n",
        "    \"downloads/annotations_trainval2014.zip\": \"annotations\",\n",
        "    \"downloads/v2_Questions_Val_mscoco.zip\": \"vqa_questions\",\n",
        "    \"downloads/v2_Annotations_Val_mscoco.zip\": \"vqa_annotations\",\n",
        "}\n",
        "\n",
        "for zip_path, extract_to in unzip_targets.items():\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"Unzipping {zip_path} to {extract_to}...\")\n",
        "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(extract_to)\n",
        "        print(f\"Extracted to {extract_to}/\")\n",
        "    else:\n",
        "        print(f\"Already extracted: {extract_to}/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 277,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "flan_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "flan_tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 278,
      "metadata": {},
      "outputs": [],
      "source": [
        "COCO_PATH = \"val2014\"\n",
        "N = 500  # Change to None for full run\n",
        "IMAGE_SIZE = (224, 224)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 279,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load VQA questions + answers\n",
        "with open(\"vqa_questions/v2_OpenEnded_mscoco_val2014_questions.json\") as f:\n",
        "    questions = json.load(f)[\"questions\"]\n",
        "\n",
        "with open(\"vqa_annotations/v2_mscoco_val2014_annotations.json\") as f:\n",
        "    annotations = json.load(f)[\"annotations\"]\n",
        "\n",
        "answers_map = {ann[\"question_id\"]: ann[\"answers\"] for ann in annotations}\n",
        "vqa = []\n",
        "for q in questions:\n",
        "    if q[\"question_id\"] in answers_map:\n",
        "        vqa.append({\n",
        "            \"image_id\": q[\"image_id\"],\n",
        "            \"question\": q[\"question\"],\n",
        "            \"answers\": answers_map[q[\"question_id\"]]\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 280,
      "metadata": {},
      "outputs": [],
      "source": [
        "rouge = RougeScorer([\"rougeL\"], use_stemmer=True)\n",
        "smoother = SmoothingFunction().method1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 281,
      "metadata": {},
      "outputs": [],
      "source": [
        "bleu_vals, rouge_vals = [], []\n",
        "bert_preds = []\n",
        "bert_refs = []\n",
        "output_log = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [],
      "source": [
        "def rearrange_question(question):\n",
        "    doc = nlp(question)\n",
        "\n",
        "    rearranged = []\n",
        "    root = None\n",
        "    root_pos = -1\n",
        "    count = 0\n",
        "    root_added = False\n",
        "    for token in doc:\n",
        "\n",
        "        # print(token.text, token.pos_)\n",
        "\n",
        "        if root_pos==-1 and token.pos_ == 'AUX':\n",
        "            if count == 0:\n",
        "                return[]\n",
        "            root = token\n",
        "            root_pos = count\n",
        "\n",
        "        elif root_pos!=-1 and token.pos_ != 'PUNCT':\n",
        "\n",
        "            if token.pos_ == 'VERB':\n",
        "                root_added = True\n",
        "                rearranged.append(root)\n",
        "            rearranged.append(token)\n",
        "\n",
        "        count += 1\n",
        "\n",
        "    if not root_added and root:\n",
        "        rearranged.append(root)\n",
        "\n",
        "    # if rearranged[-1].pos_ == 'VERB':\n",
        "    #     rearranged.insert(len(rearranged)-1, root)\n",
        "    # else:\n",
        "    #     rearranged.insert(len(rearranged), root)\n",
        "        \n",
        "    rearranged_str = [text.text.lower() for text in rearranged]\n",
        "\n",
        "    if root_pos == 2:\n",
        "        rearranged_str.insert(0, doc[1].text)\n",
        "        rearranged_str.insert(1, \"of\")\n",
        "\n",
        "    if doc[0].text == 'Why':\n",
        "        rearranged_str.append('because')\n",
        "    return rearranged_str\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_preprocess(img_path):\n",
        "    img = Image.open(img_path).convert(\"RGB\").resize(IMAGE_SIZE)\n",
        "    arr = np.asarray(img).astype(\"float32\")\n",
        "    return arr\n",
        "\n",
        "def answer_question_from_captions(captions, question):\n",
        "    prompt = f\"Given the following descriptions:\\n{' '.join(captions)}\\nAnswer the question: {question}. Donot hallucinate. Provide short answer.\"\n",
        "    inputs = flan_tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n",
        "    outputs = flan_model.generate(**inputs, max_length=40)\n",
        "    return flan_tokenizer.decode(outputs[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TensorShape([224, 224, 3])"
            ]
          },
          "execution_count": 263,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img_path = os.path.join(COCO_PATH, f\"COCO_val2014_{vqa[480]['image_id']:012d}.jpg\")\n",
        "image = load_image(img_path)\n",
        "\n",
        "im=Image.open(img_path)\n",
        "im.show()\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:908: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 2, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'causal_self_attention_2' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_layer_2' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'a white and white dog is in the air'"
            ]
          },
          "execution_count": 264,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.simple_gen(image, temperature=0.0, initial_keywords=[])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating 500 examples...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/500 [00:00<?, ?it/s]/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'causal_self_attention_7' (of type CausalSelfAttention) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "/Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/layers/layer.py:940: UserWarning: Layer 'decoder_layer_7' (of type DecoderLayer) was passed an input with a mask attached to it. However, this layer does not support masking and will therefore destroy the mask information. Downstream layers will not see the mask.\n",
            "  warnings.warn(\n",
            "  0%|          | 1/500 [00:10<1:27:29, 10.52s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "\n",
            "Q: Where is he looking?\n",
            "Keywords: ['he', 'is', 'looking']\n",
            "Caption: he is looking at a man in a black shirt\n",
            "Prediction: at a man in a black shirt\n",
            "GT: down\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 3/500 [00:11<21:59,  2.66s/it]  /Users/HP/Documents/Documents - Mac/CS 6384 Computer Vision/project-cv/llm-based-VQA-system/.venv/lib/python3.11/site-packages/keras/src/ops/nn.py:908: UserWarning: You are using a softmax over axis 3 of a tensor of shape (1, 4, 1, 1). This axis has size 1. The softmax operation will always return the value 1, which is likely not what you intended. Did you mean to use a sigmoid instead?\n",
            "  warnings.warn(\n",
            "  2%|▏         | 11/500 [00:14<03:49,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "\n",
            "Q: Why is there a gap between the roof and wall?\n",
            "Keywords: ['there', 'a', 'gap', 'between', 'the', 'roof', 'and', 'wall', 'is', 'because']\n",
            "Caption: there a gap between the roof and wall is because\n",
            "Prediction: Do not hallucinate\n",
            "GT: ventilation\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4%|▍         | 21/500 [00:19<03:38,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "20\n",
            "\n",
            "Q: What color is the bedspread?\n",
            "Keywords: ['color', 'of', 'the', 'bedspread', 'is']\n",
            "Caption: color of the bedspread is laying on a bed\n",
            "Prediction: hallucinate\n",
            "GT: beige\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  6%|▌         | 31/500 [00:23<03:16,  2.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "\n",
            "Q: Is the bed white?\n",
            "Keywords: []\n",
            "Caption: a man is sitting on a bed with a large blue blanket\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  8%|▊         | 41/500 [00:28<03:35,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40\n",
            "\n",
            "Q: How many frames are on the wall?\n",
            "Keywords: ['on', 'the', 'wall', 'are']\n",
            "Caption: on the wall are sitting on a bed\n",
            "Prediction: two\n",
            "GT: 7\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 10%|█         | 51/500 [00:32<03:27,  2.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "50\n",
            "\n",
            "Q: Is this person wearing a tie?\n",
            "Keywords: []\n",
            "Caption: a young boy is eating a small child with a red shirt\n",
            "Prediction: Yes\n",
            "GT: no\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 12%|█▏        | 61/500 [00:37<03:20,  2.19it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60\n",
            "\n",
            "Q: What kind of vehicle is the RV pulling on the bottom picture?\n",
            "Keywords: ['the', 'rv', 'is', 'pulling', 'on', 'the', 'bottom', 'picture']\n",
            "Caption: the rv is pulling on the bottom picture of a car\n",
            "Prediction: car\n",
            "GT: jeep\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 14%|█▍        | 71/500 [00:42<03:23,  2.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70\n",
            "\n",
            "Q: What does the last sign say?\n",
            "Keywords: ['the', 'last', 'sign', 'does', 'say']\n",
            "Caption: the last sign does say on the train\n",
            "Prediction: Do not hallucinate\n",
            "GT: ross st\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 16%|█▌        | 81/500 [00:46<03:09,  2.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "80\n",
            "\n",
            "Q: Is there a tree in front of the building?\n",
            "Keywords: []\n",
            "Caption: a man is standing on a street with a large building in the background\n",
            "Prediction: Yes\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 18%|█▊        | 91/500 [00:51<03:14,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "90\n",
            "\n",
            "Q: Is it still snowing in the picture?\n",
            "Keywords: []\n",
            "Caption: a man is walking through the snow\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 20%|██        | 101/500 [01:03<03:33,  1.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100\n",
            "\n",
            "Q: What is the background metal structure?\n",
            "Keywords: ['the', 'background', 'metal', 'structure', 'is']\n",
            "Caption: the background metal structure is walking down the road\n",
            "Prediction: a human\n",
            "GT: trees\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 22%|██▏       | 111/500 [01:08<03:01,  2.14it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "110\n",
            "\n",
            "Q: What year is the car?\n",
            "Keywords: ['year', 'of', 'the', 'car', 'is']\n",
            "Caption: year of the car is standing on the street\n",
            "Prediction: a year\n",
            "GT: 2014\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 24%|██▍       | 121/500 [01:13<04:14,  1.49it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "120\n",
            "\n",
            "Q: Is that a cake?\n",
            "Keywords: []\n",
            "Caption: a man in a red shirt and a black shirt is standing on a city street\n",
            "Prediction: No\n",
            "GT: no\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 26%|██▌       | 131/500 [01:19<03:32,  1.74it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "130\n",
            "\n",
            "Q: Will these giraffes be eating the grass for dinner?\n",
            "Keywords: []\n",
            "Caption: a man in a brown shirt is riding a bike on a sandy beach\n",
            "Prediction: No\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 28%|██▊       | 141/500 [01:24<02:23,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "140\n",
            "\n",
            "Q: Is this man snowboarding?\n",
            "Keywords: []\n",
            "Caption: a snowboarder in the air\n",
            "Prediction: Yes\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 30%|███       | 151/500 [01:27<02:03,  2.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "150\n",
            "\n",
            "Q: Does the man have goggles on?\n",
            "Keywords: []\n",
            "Caption: a snowboarder in the air\n",
            "Prediction: Yes\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 32%|███▏      | 161/500 [01:31<02:17,  2.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "160\n",
            "\n",
            "Q: Does this dog have a collar?\n",
            "Keywords: []\n",
            "Caption: a young boy is playing with a dog on a leash\n",
            "Prediction: Yes\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 34%|███▍      | 171/500 [01:36<02:38,  2.08it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "170\n",
            "\n",
            "Q: What color is the ribbon?\n",
            "Keywords: ['color', 'of', 'the', 'ribbon', 'is']\n",
            "Caption: color of the ribbon is sitting in a window\n",
            "Prediction: color\n",
            "GT: green\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 36%|███▌      | 181/500 [01:46<02:46,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "180\n",
            "\n",
            "Q: How many babies are there?\n",
            "Keywords: ['there', 'are']\n",
            "Caption: there are two men and a woman are playing soccer\n",
            "Prediction: two\n",
            "GT: 0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 38%|███▊      | 191/500 [01:50<02:05,  2.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "190\n",
            "\n",
            "Q: What does the woman have around her neck?\n",
            "Keywords: ['the', 'woman', 'does', 'have', 'around', 'her', 'neck']\n",
            "Caption: the woman does have around her neck\n",
            "Prediction: a ring\n",
            "GT: necklace\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 40%|████      | 201/500 [01:55<02:06,  2.37it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "200\n",
            "\n",
            "Q: What color is the court?\n",
            "Keywords: ['color', 'of', 'the', 'court', 'is']\n",
            "Caption: color of the court is running on the court\n",
            "Prediction: hallucinate\n",
            "GT: green\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 42%|████▏     | 211/500 [01:59<02:15,  2.13it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "210\n",
            "\n",
            "Q: Is the silver bike new?\n",
            "Keywords: []\n",
            "Caption: a man in a red helmet is riding a motorcycle on a dirt bike\n",
            "Prediction: No\n",
            "GT: no\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 44%|████▍     | 221/500 [02:04<02:06,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "220\n",
            "\n",
            "Q: What color is the bike?\n",
            "Keywords: ['color', 'of', 'the', 'bike', 'is']\n",
            "Caption: color of the bike is driving a green motorcycle\n",
            "Prediction: green\n",
            "GT: white and green\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 46%|████▌     | 231/500 [02:08<02:08,  2.10it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "230\n",
            "\n",
            "Q: What color is the fridge?\n",
            "Keywords: ['color', 'of', 'the', 'fridge', 'is']\n",
            "Caption: color of the [UNK] is looking at the window\n",
            "Prediction: unanswerable\n",
            "GT: gray or silver\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 48%|████▊     | 241/500 [02:14<02:39,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "240\n",
            "\n",
            "Q: What is written on the surfboard?\n",
            "Keywords: ['is', 'written', 'on', 'the', 'surfboard']\n",
            "Caption: is written on the surfboard on top of a snowy mountain\n",
            "Prediction: Do not hallucinate\n",
            "GT: lifeguard\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 50%|█████     | 251/500 [02:19<02:09,  1.92it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "250\n",
            "\n",
            "Q: Is this an English saddle?\n",
            "Keywords: []\n",
            "Caption: a woman in a pink shirt is holding a guitar\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 52%|█████▏    | 261/500 [02:24<01:35,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "260\n",
            "\n",
            "Q: What kind of birds are these?\n",
            "Keywords: ['these', 'are']\n",
            "Caption: these are flying over the beach\n",
            "Prediction: birds\n",
            "GT: herons\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 54%|█████▍    | 271/500 [02:28<01:50,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "270\n",
            "\n",
            "Q: How many sets of doors are open?\n",
            "Keywords: ['open', 'are']\n",
            "Caption: open are standing on a train\n",
            "Prediction: two sets\n",
            "GT: 1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 56%|█████▌    | 281/500 [02:35<03:00,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "280\n",
            "\n",
            "Q: Is there a woman sitting on this bench??\n",
            "Keywords: []\n",
            "Caption: a woman in a black shirt and a woman in a black shirt and a black shirt and a woman are walking on the sidewalk\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 58%|█████▊    | 291/500 [02:46<03:15,  1.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "290\n",
            "\n",
            "Q: Is the girl smiling?\n",
            "Keywords: []\n",
            "Caption: a man in a black hat and a woman in a black shirt and a black hat\n",
            "Prediction: No\n",
            "GT: no\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 60%|██████    | 301/500 [02:55<02:59,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "300\n",
            "\n",
            "Q: What about this man's appearance might bother a conservative employer?\n",
            "Keywords: ['might', 'bother', 'a', 'conservative', 'employer']\n",
            "Caption: [UNK] [UNK] a [UNK] [UNK]\n",
            "Prediction: a\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 62%|██████▏   | 311/500 [03:08<03:19,  1.06s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "310\n",
            "\n",
            "Q: Where are the ducks?\n",
            "Keywords: ['the', 'ducks', 'are']\n",
            "Caption: the ducks are walking down the street\n",
            "Prediction: street\n",
            "GT: can't see\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 64%|██████▍   | 321/500 [03:17<02:41,  1.11it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "320\n",
            "\n",
            "Q: Is this a tour bus?\n",
            "Keywords: []\n",
            "Caption: a man is riding a skateboard on a street\n",
            "Prediction: No\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 66%|██████▌   | 331/500 [03:26<02:24,  1.17it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "330\n",
            "\n",
            "Q: What color is the camper?\n",
            "Keywords: ['color', 'of', 'the', 'camper', 'is']\n",
            "Caption: color of the camper is walking down the street\n",
            "Prediction: hallucinate\n",
            "GT: white\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 68%|██████▊   | 341/500 [03:35<02:11,  1.21it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "340\n",
            "\n",
            "Q: Is the person on the laptop married?\n",
            "Keywords: []\n",
            "Caption: a man is sitting on a table with his hand\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 70%|███████   | 351/500 [03:43<01:44,  1.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "350\n",
            "\n",
            "Q: Is the giraffes head small?\n",
            "Keywords: []\n",
            "Caption: a dog is jumping over a wooden fence\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 72%|███████▏  | 361/500 [03:51<01:45,  1.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "360\n",
            "\n",
            "Q: Is the catcher wearing safety gear?\n",
            "Keywords: []\n",
            "Caption: a boy in a blue shirt is running on a field\n",
            "Prediction: No\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 74%|███████▍  | 371/500 [03:56<01:05,  1.96it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "370\n",
            "\n",
            "Q: What is the color of the sky?\n",
            "Keywords: ['the', 'color', 'of', 'the', 'sky', 'is']\n",
            "Caption: the color of the sky is flying through the air\n",
            "Prediction: hallucinate\n",
            "GT: blue\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 76%|███████▌  | 381/500 [04:04<01:34,  1.26it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "380\n",
            "\n",
            "Q: Is this going to be a feast?\n",
            "Keywords: []\n",
            "Caption: a woman is sitting in a chair with a baby in a chair\n",
            "Prediction: No\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 78%|███████▊  | 391/500 [04:10<01:01,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "390\n",
            "\n",
            "Q: What type of food is being sold?\n",
            "Keywords: ['being', 'is', 'sold']\n",
            "Caption: being is [UNK] of people in a restaurant\n",
            "Prediction: food\n",
            "GT: pastries\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 80%|████████  | 401/500 [04:17<00:54,  1.83it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "400\n",
            "\n",
            "Q: What is the large figure?\n",
            "Keywords: ['the', 'large', 'figure', 'is']\n",
            "Caption: the large figure is walking on the beach\n",
            "Prediction: a human\n",
            "GT: two giraffe\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 82%|████████▏ | 411/500 [04:23<00:50,  1.76it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "410\n",
            "\n",
            "Q: Do the people know each other?\n",
            "Keywords: []\n",
            "Caption: two children are sitting on a couch\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 84%|████████▍ | 421/500 [04:30<00:54,  1.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "420\n",
            "\n",
            "Q: Is this a glass flower vase?\n",
            "Keywords: []\n",
            "Caption: a man in a red shirt is standing on a table with a red and white bike\n",
            "Prediction: No\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 86%|████████▌ | 431/500 [04:37<00:41,  1.65it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "430\n",
            "\n",
            "Q: Is the train in the city?\n",
            "Keywords: []\n",
            "Caption: a person is walking down a road\n",
            "Prediction: No\n",
            "GT: no\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 88%|████████▊ | 441/500 [04:43<00:27,  2.18it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "440\n",
            "\n",
            "Q: What is the cat looking at?\n",
            "Keywords: ['the', 'cat', 'is', 'looking', 'at']\n",
            "Caption: the cat is looking at the camera\n",
            "Prediction: camera\n",
            "GT: camera\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 90%|█████████ | 451/500 [04:49<00:25,  1.91it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "450\n",
            "\n",
            "Q: How many rolls of toilet paper are there?\n",
            "Keywords: ['there', 'are']\n",
            "Caption: there are two people sitting on a blue blanket\n",
            "Prediction: None\n",
            "GT: 1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 92%|█████████▏| 461/500 [04:54<00:21,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "460\n",
            "\n",
            "Q: How many cats?\n",
            "Keywords: []\n",
            "Caption: a person is sitting on a blue blanket\n",
            "Prediction: a single cat\n",
            "GT: 0\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 94%|█████████▍| 471/500 [05:00<00:16,  1.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "470\n",
            "\n",
            "Q: Does this toilet have a lit?\n",
            "Keywords: []\n",
            "Caption: a person is sitting on a blue blanket\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 96%|█████████▌| 481/500 [05:07<00:11,  1.59it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "480\n",
            "\n",
            "Q: Is the bathroom clean?\n",
            "Keywords: []\n",
            "Caption: a person is sitting on a blue blanket\n",
            "Prediction: no\n",
            "GT: yes\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 98%|█████████▊| 491/500 [05:12<00:04,  2.09it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "490\n",
            "\n",
            "Q: What is the metal rack on the wall for?\n",
            "Keywords: ['the', 'metal', 'rack', 'on', 'the', 'wall', 'for', 'is']\n",
            "Caption: the metal rack on the wall for is looking at the camera\n",
            "Prediction: a camera\n",
            "GT: toilet paper holder\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 500/500 [05:17<00:00,  1.57it/s]\n"
          ]
        }
      ],
      "source": [
        "print(f\"Evaluating {N or len(vqa)} examples...\\n\")\n",
        "count = 0\n",
        "for example in tqdm.tqdm(vqa[:N]):\n",
        "    try:\n",
        "        img_id = int(example[\"image_id\"])\n",
        "        img_path = os.path.join(COCO_PATH, f\"COCO_val2014_{img_id:012d}.jpg\")\n",
        "\n",
        "        image_tensor = load_and_preprocess(img_path)\n",
        "        keywords = rearrange_question(example[\"question\"])\n",
        "        caption = model.simple_gen(image_tensor, temperature=0.0, initial_keywords=keywords)\n",
        "\n",
        "        prediction = answer_question_from_captions([caption], example[\"question\"])\n",
        "        references = [a[\"answer\"].lower() for a in example[\"answers\"]]\n",
        "\n",
        "        # BLEU & ROUGE\n",
        "        bleu = max(sentence_bleu([r.split()], prediction.split(), smoothing_function=smoother) for r in references)\n",
        "        rouge_l = max(rouge.score(prediction, ref)[\"rougeL\"].fmeasure for ref in references)\n",
        "\n",
        "        bleu_vals.append(bleu)\n",
        "        rouge_vals.append(rouge_l)\n",
        "        bert_preds.append(prediction)\n",
        "        bert_refs.append(references[0])\n",
        "\n",
        "        # Log sample\n",
        "        if len(bleu_vals) % 10 == 1:\n",
        "            print(count)\n",
        "            print(f\"\\nQ: {example['question']}\\nKeywords: {keywords}\\nCaption: {caption}\\nPrediction: {prediction}\\nGT: {references[0]}\\n\")\n",
        "\n",
        "        output_log.append({\n",
        "            \"image_id\": img_id,\n",
        "            \"question\": example[\"question\"],\n",
        "            \"caption\": caption,\n",
        "            \"prediction\": prediction,\n",
        "            \"ground_truth\": references\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(example[\"question\"])\n",
        "        print(f\"Error at {img_id}: {e}\")\n",
        "        continue\n",
        "\n",
        "    count += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {},
      "outputs": [],
      "source": [
        "# BERTScore\n",
        "_, _, bert_f1 = bert_score(bert_preds, bert_refs, lang=\"en\", model_type=\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 286,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "────────── RESULTS ──────────\n",
            "Avg BLEU-4   : 0.0305\n",
            "Avg ROUGE-L  : 0.3606\n",
            "Avg BERTScore: 0.6765\n"
          ]
        }
      ],
      "source": [
        "# Final results\n",
        "print(\"\\n────────── RESULTS ──────────\")\n",
        "print(f\"Avg BLEU-4   : {np.mean(bleu_vals):.4f}\")\n",
        "print(f\"Avg ROUGE-L  : {np.mean(rouge_vals):.4f}\")\n",
        "print(f\"Avg BERTScore: {bert_f1.mean().item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 287,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Saved results to 'vqa_eval_results.csv'\n"
          ]
        }
      ],
      "source": [
        "# Save CSV\n",
        "df = pd.DataFrame(output_log)\n",
        "df.to_csv(\"vqa_eval_results.csv\", index=False)\n",
        "print(\"\\nSaved results to 'vqa_eval_results.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "image_captioning.ipynb",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
